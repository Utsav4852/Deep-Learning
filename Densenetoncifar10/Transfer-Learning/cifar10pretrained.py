# -*- coding: utf-8 -*-
"""pretrained.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k7M5_b7xuIrfCCUzj1S5A4FtONNFGQgo
"""

# import all the libraries
import numpy as np
import sklearn.metrics as metrics
from keras.applications import densenet
from keras.datasets import cifar10
from keras.utils import np_utils
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense,Flatten,Dropout

# for removing warnings
import warnings
warnings.filterwarnings('ignore')

# define imagenet-pretrained model for densenet(cifar10)
model = Sequential()
model.add(densenet.DenseNet121(weights='imagenet', include_top=False, input_shape=(32,32,3), pooling='max'))
# adding dense layer for flatten 
model.add(Dense(256, activation='relu'))
# deactivating 50% nodes
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax')) 

# find summary
model.summary()

# Splitting traning and testing set
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Converting to float
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# converting data into normalize form
x_train = densenet.preprocess_input(x_train)
x_test = densenet.preprocess_input(x_test)

# one-hot encoding
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)

# Using Adam instead of SGD to speed up training and set learning rate 0.001
optimizer = Adam(lr=1e-3)  

# compile the model
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=["accuracy"])

# train the model
model.fit(x_train, y_train, batch_size=64, shuffle=True, epochs=20, validation_data=(x_test, y_test))

Preds = model.predict(x_test)
y_Pred = np.argmax(Preds, axis=1)
y_true = y_test

# finding accuracy and loss
accuracy = metrics.accuracy_score(y_true, y_Pred) * 100

# print testing accuracy
print("Accuracy : ", accuracy)